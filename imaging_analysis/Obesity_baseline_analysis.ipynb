{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## code was written using Python 2.7.13 ##\n",
    "import numpy as np #v1.14.4\n",
    "import seaborn as sns #v0.7.1\n",
    "import matplotlib as plt #v1.5.1\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy as sc #v0.19.0\n",
    "from scipy import stats\n",
    "import sklearn #0.18.1\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd #0.23.0\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##EDIT THIS SECTION##\n",
    "\n",
    "## open multiple files called 'extractedsignals.npy' and combine into single array.\n",
    "## the best way to do this is to point the basedir to a folder. Within that folder there are multiple folders (named to identify\n",
    "## session,mouse,FOV,etc), which each contain EXACTLY 1 file called EXACTLY 'extractedsignals.npy'\n",
    "\n",
    "%cd 'C:\\Users\\Stuber Lab\\Desktop\\Raw data\\obesity imaging'  ##point to data location ##\n",
    "\n",
    "maxnumneurons = 1000 #just used to initialize arrays. Should be larger than the total number of neurons\n",
    "maxframes=3000       #number of frames\n",
    "\n",
    "## Do you want to use F/F0 or z-scored data?\n",
    "use_raw_data=['yes'] ## if 'no' then data will be z-scored\n",
    "raw_threshold=1.1  #define raw threshold value for identifying events\n",
    "z_threshold=1.5      #define z threshold value for identifying events\n",
    "\n",
    "framerate=5 #framerate in Hz\n",
    "\n",
    "### Filter the event by duration or area under the curve (values of 0 = unfiltered)\n",
    "timethreshold=2  #threshold for duration of event (in seconds) \n",
    "areathreshold=0  #threshold for area under the curve of event\n",
    "\n",
    "#do you want to use the low-pass filter to adjust the z-score values?\n",
    "low_pass_filter_on=['no'] #either 'yes' or 'no'\n",
    "lowpass_filter=1.1 # Change this to modify the low-pass correction. (values between 1.05 and 1.1 usually work well)\n",
    "\n",
    "#do you want to PLOT all individual traces? (if you have a lot of cells, this can be pretty time consuming)\n",
    "plot_traces=['no'] ##plot unfiltered traces?\n",
    "plot_traces_filtered=['no'] ##plot filtered traces?\n",
    "\n",
    "#do you want to EXPORT individual traces? plot_traces must equal 'yes' in order to export traces.\n",
    "export_traces=['no']\n",
    "\n",
    "# do you want to export data files?\n",
    "export_files=['no']\n",
    "\n",
    "## do you want to save bar graphs?\n",
    "save_figs=['no']\n",
    "\n",
    "## if you want to limit your analysis to a certain range of frames, change start_frame and end_frame below\n",
    "start_frame=0 \n",
    "end_frame=3000\n",
    "\n",
    "bin_avg=3 #rolling average smooth; greater values increase the smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_events(basedir,filename):\n",
    "    signals_pop = np.nan*np.zeros((maxnumneurons,maxframes))\n",
    "    data_dirs = os.walk(basedir).next()[1]\n",
    "    numneuronstillnow = 0\n",
    "    for data_dir in data_dirs:\n",
    "        signals=np.load(os.path.join(basedir,data_dir,'extractedsignals.npy'))\n",
    "        numneurons=signals.shape[1]\n",
    "        numframes=signals.shape[2]\n",
    "        temp_signals=np.squeeze(signals)\n",
    "        for a in range(0,numneurons):\n",
    "            signals_pop[numneuronstillnow+a,:numframes]=temp_signals[a,:]\n",
    "        numneuronstillnow += numneurons\n",
    "    extractedsignals=signals_pop[:numneuronstillnow,:numframes]\n",
    "    print ' '\n",
    "    print filename\n",
    "    print 'files = '+str(data_dirs)\n",
    "\n",
    "    ## if you want to limit your analysis to a certain range of ROIs, change start_roi and end_roi below\n",
    "    start_roi=0\n",
    "    end_roi=extractedsignals.shape[0]\n",
    "\n",
    "    #save parameters to csv\n",
    "    params=['framerate = '+str(framerate),'z-threshold = '+str(z_threshold),'raw threshold = '+str(raw_threshold),\n",
    "            'time threshold = '+str(timethreshold),\n",
    "             'area threshold = '+str(areathreshold),'low pass filter on? = '+low_pass_filter_on[0],\n",
    "             'low pass filter = '+str(lowpass_filter),'start frame = '+str(start_frame),'end frame = '+str(end_frame),\n",
    "             'number of cells = '+str(extractedsignals.shape[0]), 'use raw data? = '+str(use_raw_data[0])]\n",
    "    with open(filename+'_parameters','wb') as myfile:\n",
    "        out=csv.writer(myfile,delimiter=',')\n",
    "        out.writerow(params)\n",
    "\n",
    "    #zscore data\n",
    "    zscore_signals=stats.zscore(extractedsignals[:,:], axis=1)\n",
    "    numrois=zscore_signals.shape[0]\n",
    "    numframes=zscore_signals.shape[1]\n",
    "    if use_raw_data[0]=='yes':\n",
    "        zscore_signals=extractedsignals\n",
    "        y_axis='F/F0'\n",
    "        threshold=raw_threshold\n",
    "    else:\n",
    "        y_axis='z-scored intensity'\n",
    "        threshold=z_threshold\n",
    "    print 'Total ROIs = '+str(numrois)\n",
    "    print 'Total Frames = '+str(numframes)\n",
    "\n",
    "    #restrict frames/ROIs to values defined above\n",
    "    zscore_signals=zscore_signals[start_roi:end_roi,start_frame:end_frame]\n",
    "    numrois=zscore_signals.shape[0]\n",
    "    numframes=zscore_signals.shape[1]\n",
    "    print 'ROIs after restriction = '+str(numrois)\n",
    "    print 'Frames after restriction = '+str(numframes)\n",
    "\n",
    "    ##re-zscore data after removing high-amplitude events\n",
    "    ## Correction is as follows:\n",
    "    ## 1.    Lowpass filter Raw SIMA traces\n",
    "    ## 2.    (Mean(raw) - Mean(filt)) / STD(raw) = Correction Constant\n",
    "    ## 3.    Zscore (raw) - Correction Constant = Corrected z-score\n",
    "    zscore_signals_filt=np.nan*np.empty(zscore_signals.shape)\n",
    "    for a in range(0,zscore_signals.shape[0]):\n",
    "        for b in range(0,zscore_signals.shape[1]):\n",
    "            if extractedsignals[a,b]>lowpass_filter:\n",
    "                zscore_signals_filt[a,b]=np.nan\n",
    "            else:\n",
    "                zscore_signals_filt[a,b]=extractedsignals[a,b]\n",
    "    zscore_signals_filt_mean=np.nanmean(zscore_signals_filt, axis=1)\n",
    "    zscore_signals_mean=np.nanmean(extractedsignals, axis=1)  \n",
    "    adjust_zscore=zscore_signals_filt_mean-zscore_signals_mean\n",
    "    extractedsignals_std=np.nanstd(extractedsignals, axis=1)\n",
    "    zscore_signals_filt_mean=np.nanmean(zscore_signals_filt, axis=1)\n",
    "    zscore_signals_filt_std=np.nanstd(extractedsignals, axis=1)\n",
    "    for a in range(0,zscore_signals.shape[0]):\n",
    "        for b in range(0,zscore_signals.shape[1]):\n",
    "            zscore_signals_filt[a,b]=(zscore_signals_filt[a,b]-zscore_signals_filt_mean[a])/(zscore_signals_filt_std[a])      \n",
    "    for a in range(0,zscore_signals.shape[0]):\n",
    "        for b in range(0,zscore_signals.shape[1]):\n",
    "            zscore_signals_filt[a,b]=zscore_signals[a,b]-(adjust_zscore[a]/extractedsignals_std[a])\n",
    "\n",
    "    ##low pass filter##\n",
    "    if low_pass_filter_on[0]=='yes':\n",
    "        zscore_signals=zscore_signals_filt  \n",
    "    else:\n",
    "        zscore_signals=zscore_signals\n",
    "    print 'low-pass filter = '+low_pass_filter_on[0]\n",
    "\n",
    "    ## rolling mean smooth on the zscored data.##\n",
    "    zscore_signals_df=pd.DataFrame(zscore_signals[:,:])\n",
    "#     signals_smoothed=pd.rolling_mean(zscore_signals_df, bin_avg, axis=1) #used for older versions of Pandas\n",
    "    signals_smoothed=zscore_signals_df.rolling(bin_avg,axis=1).mean()\n",
    "    signals_smoothed=pd.DataFrame.as_matrix(signals_smoothed)\n",
    "    signals_smoothed.shape\n",
    "\n",
    "    #### find 'up' and 'down' events.  up corresponds to rising edge\n",
    "    up=np.empty(signals_smoothed.shape)\n",
    "    up[:]=np.NAN\n",
    "    down=np.empty(signals_smoothed.shape)\n",
    "    down[:]=np.NAN\n",
    "    for a in range(0,numrois):\n",
    "        for i in range(1,numframes-1):\n",
    "            up[a,i]=signals_smoothed[a,i]>threshold and signals_smoothed[a,i-1]<threshold and signals_smoothed[a,i+1]>threshold\n",
    "            down[a,i]=signals_smoothed[a,i]>threshold and signals_smoothed[a,i-1]>threshold and signals_smoothed[a,i+1]<threshold\n",
    "\n",
    "    #plot ts with unfiltered event borders overlayed in red and export if desired\n",
    "    if plot_traces[0]=='yes':\n",
    "        uptimesthreshold=up*threshold\n",
    "        downtimesthreshold=down*threshold\n",
    "        uptimesthreshold[uptimesthreshold==0]=np.nan\n",
    "        downtimesthreshold[downtimesthreshold==0]=np.nan    \n",
    "        for a in range(0,numrois):\n",
    "            fig,ax = plt.subplots(1)\n",
    "            sns.tsplot(signals_smoothed[a,start_frame:end_frame])\n",
    "            sns.tsplot(downtimesthreshold[a,start_frame:end_frame], c='r', interpolate=False)\n",
    "            sns.tsplot(uptimesthreshold[a,start_frame:end_frame], c='c', interpolate=False)\n",
    "            plt.xlabel('Frames', fontsize='16')\n",
    "            plt.ylabel(y_axis, fontsize='16')\n",
    "            plt.title('ROI numer '+str(a), fontsize='16')\n",
    "            ax.set_axis_bgcolor('white')\n",
    "            plt.show()\n",
    "            if export_traces[0]=='yes':\n",
    "                fig.savefig(filename+'_ROI_'+str(a)+'.png', format='png') #uncomment this line to save all traces\n",
    "                fig.clf()\n",
    "\n",
    "    #Remove event if trace starts or ends in 'up' state\n",
    "    #if first nonzero element of combined is -1, then change it to 0, if the last nonzero is 1, make it zero\n",
    "    down=down*-1\n",
    "    combined=up+down #values of 1 indicates rise and -1 indicates fall\n",
    "    for a in range(0,numrois):\n",
    "        firstup=np.nanargmax(combined[a,:])\n",
    "        firstdown=np.nanargmin(combined[a,:])\n",
    "        if firstdown<firstup:\n",
    "            combined[a,firstdown]=0     \n",
    "    flipped=np.fliplr(combined)\n",
    "    for a in range(0,numrois):\n",
    "        lastup=np.nanargmax(flipped[a,:])\n",
    "        lastdown=np.nanargmin(flipped[a,:])\n",
    "        if lastup<lastdown:\n",
    "            flipped[a,lastup]=0\n",
    "    combined=np.fliplr(flipped)\n",
    "    test=abs(combined)*threshold\n",
    "    test[test==0]=np.nan\n",
    "\n",
    "    #append frame number to array\n",
    "    time=np.arange(numframes)\n",
    "    zscore_signals2=np.squeeze(signals_smoothed)\n",
    "    signals_with_time=np.vstack((zscore_signals2, time)) \n",
    "    signals_with_time[-1,:]\n",
    "    #find frame number of 'up' and 'down' events.  Up corresponds to rising edge, down=falling edge\n",
    "    diff_down=np.empty(signals_with_time.shape)\n",
    "    diff_down[:]=np.NAN\n",
    "    diff_up=np.empty(signals_with_time.shape)\n",
    "    diff_up[:]=np.NAN\n",
    "    for a in range(0,numrois):\n",
    "        for i in range(0,numframes):\n",
    "            if combined[a,i]<0:\n",
    "                diff_down[a,i]=signals_with_time[-1,i]\n",
    "            if combined[a,i]>0:\n",
    "                diff_up[a,i]=signals_with_time[-1,i]             \n",
    "    temp_up=np.empty([signals_with_time.shape[1],2])\n",
    "    temp_up[:]=np.NAN\n",
    "    temp_down=np.empty([signals_with_time.shape[1],2])\n",
    "    temp_down[:]=np.NAN\n",
    "    event_time=np.NAN*np.ones(signals_with_time.shape)\n",
    "    allevents=np.NAN*np.ones(signals_with_time.shape)\n",
    "    allareas=np.NAN*np.ones(signals_with_time.shape)\n",
    "    allpeak=np.NAN*np.ones(signals_with_time.shape)\n",
    "    \n",
    "    maximum=np.nanmax(zscore_signals,axis=1)\n",
    "    minimum=np.nanmin(zscore_signals,axis=1)\n",
    "    std=np.nanstd(zscore_signals,axis=1)\n",
    "    mean=np.nanmean(zscore_signals,axis=1)\n",
    "    \n",
    "    for a in range(0,numrois):\n",
    "        temp_up[:,0]=diff_up[a,:]\n",
    "        temp_down[:,1]=diff_down[a,:]\n",
    "        temp_up2=temp_up[~np.isnan(temp_up)]\n",
    "        temp_down2 = temp_down[~np.isnan(temp_down)]\n",
    "        event_time=temp_down2-temp_up2\n",
    "        for q in range (0,event_time.shape[0]):\n",
    "            area=np.NAN*np.ones(event_time.shape[0])\n",
    "            placehold_frames=signals_smoothed[a,int(temp_up2[q]):int(temp_down2[q])]\n",
    "            peak=np.max(placehold_frames)\n",
    "            area=np.trapz(placehold_frames)#, x=None, dx=1)#, axis=-1)   \n",
    "            allareas[a,q] = area \n",
    "            allpeak[a,q] = peak\n",
    "        allevents[a,:event_time.shape[0]] = event_time\n",
    "    allevents=allevents/framerate\n",
    "    allevents_pre_filt=allevents\n",
    "    allareas_pre_filt=allareas\n",
    "    framethreshold=timethreshold\n",
    "    count_events=~np.isnan(allevents)\n",
    "    count_events=np.sum(count_events,axis=1);count_events=count_events[0:count_events.shape[0]-1]\n",
    "    event_dur_mean=np.nanmean(allevents,axis=1)\n",
    "    auc_mean=np.nanmean(allareas,axis=1)\n",
    "    peak_mean=np.nanmean(allpeak,axis=1)\n",
    "    allareasdurfilt=allareas\n",
    "    allevents_again=allevents\n",
    "    allareas_again=allareas\n",
    "    allpeak_again=allpeak\n",
    "    ##find event times and export inter-event intervals\n",
    "    combine_time=np.vstack((up,time))\n",
    "    for a in range(0,numrois):\n",
    "        for b in range(0,numframes):\n",
    "            if combined[a,b]==1:\n",
    "                combine_time[a,b]=combine_time[-1,b]\n",
    "    event_time=np.nan*np.zeros((combined.shape))\n",
    "    event_iei=np.nan*np.zeros((combined.shape))\n",
    "    combine_time[combine_time==0]=np.nan\n",
    "    for a in range(0,numrois):\n",
    "        temp_time=combine_time[a,:]\n",
    "        temp_time=temp_time[~np.isnan(temp_time)]\n",
    "        temp_iei=np.diff(temp_time)\n",
    "        event_time[a,0:temp_time.shape[0]]=temp_time\n",
    "        event_iei[a,0:temp_iei.shape[0]]=temp_iei\n",
    "    event_iei=event_iei/framerate\n",
    "    event_iei_mean=np.nanmean(event_iei,axis=1) \n",
    "    #remove nans from event arrays arrays\n",
    "    allevents2=allevents[~np.isnan(allevents)]\n",
    "    allareas2=allareas[~np.isnan(allareas)]\n",
    "    allpeak2=allpeak[~np.isnan(allpeak)]\n",
    "    alliei=event_iei[~np.isnan(event_iei)]\n",
    "\n",
    "    #filter data based on thresholds defined above\n",
    "    allevents_pre_filt[allevents_pre_filt<framethreshold]=np.nan\n",
    "    allareas_pre_filt[allareas_pre_filt<areathreshold]=np.nan\n",
    "    allevents_filt=allevents_pre_filt[~np.isnan(allevents_pre_filt)]\n",
    "    allareas_filt=allareas_pre_filt[~np.isnan(allareas_pre_filt)]\n",
    "\n",
    "    #filter peak amplitude, auc, and iei using the filtered duration data\n",
    "    peak_filt=allpeak\n",
    "    auc_durfilt=allareasdurfilt\n",
    "    event_frames_durfilt=event_time\n",
    "    iei_durfilt=np.nan*np.zeros((allevents.shape))\n",
    "    event_frames_durfilt=np.vstack((event_frames_durfilt,(np.nan*np.zeros((1,numframes)))))\n",
    "\n",
    "    for a in range(0,allevents.shape[0]):\n",
    "        for b in range(0,allevents.shape[1]):\n",
    "            if allevents[a,b]>0:\n",
    "                peak_filt[a,b]=peak_filt[a,b]\n",
    "                auc_durfilt[a,b]=auc_durfilt[a,b]\n",
    "                event_frames_durfilt[a,b]=event_frames_durfilt[a,b]\n",
    "            else:\n",
    "                peak_filt[a,b]=np.nan \n",
    "                auc_durfilt[a,b]=np.nan\n",
    "                event_frames_durfilt[a,b]=np.nan\n",
    "    peak_filt_mean=np.nanmean(peak_filt,axis=1)\n",
    "    auc_durfilt_mean=np.nanmean(auc_durfilt,axis=1)\n",
    "    for a in range(0,numrois):\n",
    "        temp_frame_nums=event_frames_durfilt[a,:]\n",
    "        temp_frame_nums=temp_frame_nums[~np.isnan(temp_frame_nums)]\n",
    "        temp_iei=np.diff(temp_frame_nums)\n",
    "        iei_durfilt[a,0:temp_iei.shape[0]]=temp_iei\n",
    "    iei_durfilt=iei_durfilt/framerate\n",
    "    iei_durfilt_mean=np.nanmean(iei_durfilt,axis=1)\n",
    "\n",
    "    print 'Number of events = '+str(allevents_filt.shape[0])\n",
    "    allpeakfilt=peak_filt[~np.isnan(peak_filt)]\n",
    "    alliei_durfilt=iei_durfilt[~np.isnan(iei_durfilt)]\n",
    "    count_dur_filt=~np.isnan(allevents)\n",
    "    count_dur_filt=np.sum(count_dur_filt,axis=1);count_dur_filt=count_dur_filt[0:count_dur_filt.shape[0]-1]\n",
    "    count_AUC_filt=~np.isnan(allareas)\n",
    "    count_AUC_filt=np.sum(count_AUC_filt,axis=1);count_AUC_filt=count_AUC_filt[0:count_AUC_filt.shape[0]-1]\n",
    "    dur_filt_mean=np.nanmean(allevents,axis=1)\n",
    "    auc_filt_mean=np.nanmean(allareas,axis=1)\n",
    "\n",
    "    ## save filtered data (Figs. S3e-g) ##\n",
    "    if export_files[0]=='yes':\n",
    "        np.savetxt(filename+'_dur_filt_Avg.csv',dur_filt_mean,delimiter=',')\n",
    "        np.savetxt(filename+'_AUC_filt_Avg.csv',auc_filt_mean,delimiter=',')\n",
    "        np.savetxt(filename+'_Num_events_dur_filt.csv',count_dur_filt,delimiter=',',fmt='%i')\n",
    "        np.savetxt(filename+'_Amplitude_filt_AVG.csv',peak_filt_mean,delimiter=',')\n",
    "        np.savetxt(filename+'_iei_durfilt_Avg.csv',iei_durfilt_mean,delimiter=',')\n",
    "    return dur_filt_mean[:-1],peak_filt_mean[:-1],iei_durfilt_mean[:-1],auc_filt_mean[:-1],count_dur_filt,maximum,minimum,std,mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### find events and plot for Fig. S8a-c ###\n",
    "\n",
    "## find events for Control group 0 weeks ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry all neurons\\\\baseline\\\\Pre\\\\control-pre'\n",
    "filename='TEST_BL_C_0wks'   #Used as prefix for files created by this program\n",
    "C_0wks_dur,C_0wks_amp,C_0wks_iei,C_0wks_auc,C_0wks_count,C_0wks_max,C_0wks_min,C_0wks_std,C_0wks_mean=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for High Fat group 0 weeks ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry all neurons\\\\baseline\\\\Pre\\\\high fat-pre'\n",
    "filename='TEST_BL_HF_0wks'   #Used as prefix for files created by this program\n",
    "HF_0wks_dur,HF_0wks_amp,HF_0wks_iei,HF_0wks_auc,HF_0wks_count,HF_0wks_max,HF_0wks_min,HF_0wks_std,HF_0wks_mean=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for Control group 2 weeks ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry all neurons\\\\baseline\\\\2wks\\\\control 2wks'\n",
    "filename='TEST_BL_C_2wks'   #Used as prefix for files created by this program\n",
    "C_2wks_dur,C_2wks_amp,C_2wks_iei,C_2wks_auc,C_2wks_count,C_2wks_max,C_2wks_min,C_2wks_std,C_2wks_mean=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for High Fat group 2 weeks ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry all neurons\\\\baseline\\\\2wks\\\\HF 2wks'\n",
    "filename='TEST_BL_HF_2wks'   #Used as prefix for files created by this program\n",
    "HF_2wks_dur,HF_2wks_amp,HF_2wks_iei,HF_2wks_auc,HF_2wks_count,HF_2wks_max,HF_2wks_min,HF_2wks_std,HF_2wks_mean=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for Control group 12 weeks ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry all neurons\\\\baseline\\\\12wks\\\\control 12wks'\n",
    "filename='TEST_BL_C_12wks'   #Used as prefix for files created by this program\n",
    "C_12wks_dur,C_12wks_amp,C_12wks_iei,C_12wks_auc,C_12wks_count,C_12wks_max,C_12wks_min,C_12wks_std,C_12wks_mean=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for High Fat group 12 weeks ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry all neurons\\\\baseline\\\\12wks\\\\HF 12wks'\n",
    "filename='TEST_BL_HF_12wks'   #Used as prefix for files created by this program\n",
    "HF_12wks_dur,HF_12wks_amp,HF_12wks_iei,HF_12wks_auc,HF_12wks_count,HF_12wks_max,HF_12wks_min,HF_12wks_std,HF_12wks_mean=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## manually calculate means and sems to go into bar plot (Figs. S8a-c) ##\n",
    "condition=['control','high fat']\n",
    "def generate_bars(c1,c2,c3,hf1,hf2,hf3,label,ylim):\n",
    "    means=np.nan*np.zeros((2,3))\n",
    "    sem=np.nan*np.zeros((2,3))\n",
    "    c1=c1[~np.isnan(c1)];c2=c2[~np.isnan(c2)];c3=c3[~np.isnan(c3)]\n",
    "    hf1=hf1[~np.isnan(hf1)];hf2=hf2[~np.isnan(hf2)];hf3=hf3[~np.isnan(hf3)]\n",
    "    sem[0,0]=stats.sem(c1,ddof=0)\n",
    "    sem[0,1]=stats.sem(c2,ddof=0)\n",
    "    sem[0,2]=stats.sem(c3,ddof=0)\n",
    "    sem[1,0]=stats.sem(hf1,ddof=0)\n",
    "    sem[1,1]=stats.sem(hf2,ddof=0)\n",
    "    sem[1,2]=stats.sem(hf3,ddof=0)\n",
    "    means[0,0]= np.nanmean(c1)\n",
    "    means[0,1]= np.nanmean(c2)\n",
    "    means[0,2]= np.nanmean(c3)\n",
    "    means[1,0]= np.nanmean(hf1)\n",
    "    means[1,1]= np.nanmean(hf2)\n",
    "    means[1,2]= np.nanmean(hf3)\n",
    "\n",
    "    ind = (0,1,2)\n",
    "    ind2=(0.4,1.4,2.4)\n",
    "    width = 0.4\n",
    "    fig_bars,ax=plt.subplots(1,figsize=(3,6))\n",
    "    plt.bar=ax.bar(ind,means[0,:],width,yerr=sem[0,:],color='g',error_kw={'ecolor':'black','linewidth':2})\n",
    "    plt.bar1=ax.bar(ind2,means[1,:],width,yerr=sem[1,:],color='darkorange',error_kw={'ecolor':'black','linewidth':2})\n",
    "    ax.legend((plt.bar[0],plt.bar1[1]),(condition[0],condition[1]),loc=[1,.5],fontsize=20)\n",
    "    ax.set_axis_bgcolor('white')\n",
    "    plt.xticks((0,1.2,2.2),(0,2,12),fontsize=20)\n",
    "    plt.xlabel('Weeks',fontsize=20)\n",
    "    plt.ylabel(label,fontsize=20)\n",
    "    if ylim[1]-ylim[0]>100:\n",
    "        plt.yticks((range(ylim[0],ylim[1]+1,50)),fontsize=20)\n",
    "    else:\n",
    "        plt.yticks((range(ylim[0],ylim[1]+1)),fontsize=20)\n",
    "    ax.set_ylim(ylim[0],ylim[1])\n",
    "    if save_figs[0]=='yes':\n",
    "        fig_bars.savefig((filename+label+'_bar.png'), format='png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return means,sem\n",
    "    \n",
    "dur_mean,dur_sem=generate_bars(C_0wks_dur,C_2wks_dur,C_12wks_dur,HF_0wks_dur,HF_2wks_dur,HF_12wks_dur,label='Event_duration',ylim=[0,5])\n",
    "amp_mean,amp_sem=generate_bars(C_0wks_amp,C_2wks_amp,C_12wks_amp,HF_0wks_amp,HF_2wks_amp,HF_12wks_amp,label='Event_amplitude',ylim=[0,2])\n",
    "iei_mean,iei_sem=generate_bars(C_0wks_iei,C_2wks_iei,C_12wks_iei,HF_0wks_iei,HF_2wks_iei,HF_12wks_iei,label='Inter_event_interval',ylim=[0,150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### find events and plot for Fig. S8h-j ###\n",
    "\n",
    "\n",
    "## find events for Control group 0 weeks TRACKED ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry tracked\\\\Baseline data\\\\Pre\\\\control-pre'\n",
    "filename='TEST_BL_C_0wks_TRACKED'   #Used as prefix for files created by this program\n",
    "# C_0wks_dur_tracked,C_0wks_amp_tracked,C_0wks_iei_tracked=find_events(basedir=basedir,filename=filename)\n",
    "C_0wks_dur_tracked,C_0wks_amp_tracked,C_0wks_iei_tracked,C_0wks_auc_tracked,C_0wks_count_tracked,C_0wks_max_tracked, \\\n",
    "C_0wks_min_tracked,C_0wks_std_tracked,C_0wks_mean_tracked=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for High Fat group 0 weeks TRACKED ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry tracked\\\\Baseline data\\\\Pre\\\\high fat-pre'\n",
    "filename='TEST_BL_HF_0wks_TRACKED'   #Used as prefix for files created by this program\n",
    "HF_0wks_dur_tracked,HF_0wks_amp_tracked,HF_0wks_iei_tracked,HF_0wks_auc_tracked,HF_0wks_count_tracked,HF_0wks_max_tracked, \\\n",
    "HF_0wks_min_tracked,HF_0wks_std_tracked,HF_0wks_mean_tracked=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for Control group 2 weeks TRACKED ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry tracked\\\\Baseline data\\\\2wks\\\\control 2wks'\n",
    "filename='TEST_BL_C_2wks_TRACKED'   #Used as prefix for files created by this program\n",
    "C_2wks_dur_tracked,C_2wks_amp_tracked,C_2wks_iei_tracked,C_2wks_auc_tracked,C_2wks_count_tracked,C_2wks_max_tracked, \\\n",
    "C_2wks_min_tracked,C_2wks_std_tracked,C_2wks_mean_tracked=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for High Fat group 2 weeks TRACKED ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry tracked\\\\Baseline data\\\\2wks\\\\HF 2wks'\n",
    "filename='TEST_BL_HF_2wks_TRACKED'   #Used as prefix for files created by this program\n",
    "HF_2wks_dur_tracked,HF_2wks_amp_tracked,HF_2wks_iei_tracked,HF_2wks_auc_tracked,HF_2wks_count_tracked,HF_2wks_max_tracked, \\\n",
    "HF_2wks_min_tracked,HF_2wks_std_tracked,HF_2wks_mean_tracked=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for Control group 12 weeks TRACKED ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry tracked\\\\Baseline data\\\\12wks\\\\control 12wks'\n",
    "filename='TEST_BL_C_12wks_TRACKED'   #Used as prefix for files created by this program\n",
    "C_12wks_dur_tracked,C_12wks_amp_tracked,C_12wks_iei_tracked,C_12wks_auc_tracked,C_12wks_count_tracked,C_12wks_max_tracked, \\\n",
    "C_12wks_min_tracked,C_12wks_std_tracked,C_12wks_mean_tracked=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## find events for High Fat group 12 weeks TRACKED ##\n",
    "basedir = '\\\\Users\\\\Stuber Lab\\\\Desktop\\\\Raw data\\\\obesity imaging\\\\hungry tracked\\\\Baseline data\\\\12wks\\\\HF 12wks'\n",
    "filename='TEST_BL_HF_12wks_TRACKED'   #Used as prefix for files created by this program\n",
    "HF_12wks_dur_tracked,HF_12wks_amp_tracked,HF_12wks_iei_tracked,HF_12wks_auc_tracked,HF_12wks_count_tracked,HF_12wks_max_tracked, \\\n",
    "HF_12wks_min_tracked,HF_12wks_std_tracked,HF_12wks_mean_tracked=find_events(basedir=basedir,filename=filename)\n",
    "\n",
    "## Normalize, calculate means and sems to go into bar plot (Figs. S5d-f) ##\n",
    "condition=['control','high fat']\n",
    "def generate_bars_norm(c1,c2,c3,hf1,hf2,hf3,label,ylim):\n",
    "    c1_norm=(c1/float(np.nanmean(c1)))*100\n",
    "    c2_norm=(c2/float(np.nanmean(c1)))*100\n",
    "    c3_norm=(c3/float(np.nanmean(c1)))*100\n",
    "    hf1_norm=(hf1/float(np.nanmean(hf1)))*100\n",
    "    hf2_norm=(hf2/float(np.nanmean(hf1)))*100\n",
    "    hf3_norm=(hf3/float(np.nanmean(hf1)))*100\n",
    "    means=np.nan*np.zeros((2,3))\n",
    "    sem=np.nan*np.zeros((2,3))\n",
    "    c1_norm=c1_norm[~np.isnan(c1_norm)]\n",
    "    c2_norm=c2_norm[~np.isnan(c2_norm)]\n",
    "    c3_norm=c3_norm[~np.isnan(c3_norm)]\n",
    "    hf1_norm=hf1_norm[~np.isnan(hf1_norm)]\n",
    "    hf2_norm=hf2_norm[~np.isnan(hf2_norm)]\n",
    "    hf3_norm=hf3_norm[~np.isnan(hf3_norm)]\n",
    "    sem[0,0]=stats.sem(c1_norm,ddof=0)\n",
    "    sem[0,1]=stats.sem(c2_norm,ddof=0)\n",
    "    sem[0,2]=stats.sem(c3_norm,ddof=0)\n",
    "    sem[1,0]=stats.sem(hf1_norm,ddof=0)\n",
    "    sem[1,1]=stats.sem(hf2_norm,ddof=0)\n",
    "    sem[1,2]=stats.sem(hf3_norm,ddof=0)\n",
    "    means[0,0]= np.nanmean(c1_norm)\n",
    "    means[0,1]= np.nanmean(c2_norm)\n",
    "    means[0,2]= np.nanmean(c3_norm)\n",
    "    means[1,0]= np.nanmean(hf1_norm)\n",
    "    means[1,1]= np.nanmean(hf2_norm)\n",
    "    means[1,2]= np.nanmean(hf3_norm)\n",
    "\n",
    "    ind = (0,1,2)\n",
    "    ind2=(0.4,1.4,2.4)\n",
    "    width = 0.4\n",
    "    fig_bars,ax=plt.subplots(1,figsize=(3,6))\n",
    "    plt.bar=ax.bar(ind,means[0,:],width,yerr=sem[0,:],color='g',error_kw={'ecolor':'black','linewidth':2})\n",
    "    plt.bar1=ax.bar(ind2,means[1,:],width,yerr=sem[1,:],color='darkorange',error_kw={'ecolor':'black','linewidth':2})\n",
    "    ax.legend((plt.bar[0],plt.bar1[1]),(condition[0],condition[1]),loc=[1,.5],fontsize=20)\n",
    "    ax.set_axis_bgcolor('white')\n",
    "    plt.xticks((0,1.2,2.2),(0,2,12),fontsize=20)\n",
    "    plt.xlabel('Weeks',fontsize=20)\n",
    "    plt.ylabel(label,fontsize=20)\n",
    "    if ylim[1]-ylim[0]>100:\n",
    "        plt.yticks((range(ylim[0],ylim[1]+1,50)),fontsize=20)\n",
    "    else:\n",
    "        plt.yticks((range(ylim[0],ylim[1]+1)),fontsize=20)\n",
    "    ax.set_ylim(ylim[0],ylim[1])\n",
    "    if save_figs[0]=='yes':\n",
    "        fig_bars.savefig((filename+label+'_bar.png'), format='png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return means,sem\n",
    "\n",
    "dur_mean_tracked,dur_sem_tracked=generate_bars_norm(C_0wks_dur_tracked,C_2wks_dur_tracked,C_12wks_dur_tracked,\n",
    "                                               HF_0wks_dur_tracked,HF_2wks_dur_tracked,HF_12wks_dur_tracked,\n",
    "                                               label='Norm. Event_duration',ylim=[0,200])\n",
    "amp_mean_tracked,amp_sem_tracked=generate_bars_norm(C_0wks_amp_tracked,C_2wks_amp_tracked,C_12wks_amp_tracked,\n",
    "                                                    HF_0wks_amp_tracked,HF_2wks_amp_tracked,HF_12wks_amp_tracked,\n",
    "                                                    label='Norm. Event_amplitude',ylim=[0,200])\n",
    "iei_mean_tracked,iei_sem_tracked=generate_bars_norm(C_0wks_iei_tracked,C_2wks_iei_tracked,C_12wks_iei_tracked,\n",
    "                                                    HF_0wks_iei_tracked,HF_2wks_iei_tracked,HF_12wks_iei_tracked,\n",
    "                                                    label='Norm. Inter_event_interval',ylim=[0,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## create dataframes for SVM ##\n",
    "def create_df(amp,auc,condition,dur,group,iei,maximum,mean,minimum,num,std,timepoint):\n",
    "    df=pd.DataFrame()\n",
    "    df['amp']=amp\n",
    "    df['auc']=auc\n",
    "    df['condition']=condition\n",
    "    df['dur']=dur\n",
    "    df['group']=group\n",
    "    df['iei']=iei\n",
    "    df['max']=maximum\n",
    "    df['mean']=mean\n",
    "    df['min']=minimum\n",
    "    df['num events']=num\n",
    "    df['std']=std\n",
    "    df['timepoint']=timepoint\n",
    "    return df\n",
    "\n",
    "df_C0H=create_df(C_0wks_amp,C_0wks_auc,1,C_0wks_dur,0,C_0wks_iei,C_0wks_max,C_0wks_mean,C_0wks_min,C_0wks_count,C_0wks_std,0)\n",
    "df_C2H=create_df(C_2wks_amp,C_2wks_auc,1,C_2wks_dur,0,C_2wks_iei,C_2wks_max,C_2wks_mean,C_2wks_min,C_2wks_count,C_2wks_std,2)\n",
    "df_C12H=create_df(C_12wks_amp,C_12wks_auc,1,C_12wks_dur,0,C_12wks_iei,C_12wks_max,C_12wks_mean,C_12wks_min,C_12wks_count,C_12wks_std,12)\n",
    "\n",
    "df_HF0H=create_df(HF_0wks_amp,HF_0wks_auc,1,HF_0wks_dur,1,HF_0wks_iei,HF_0wks_max,HF_0wks_mean,HF_0wks_min,HF_0wks_count,HF_0wks_std,0)\n",
    "df_HF2H=create_df(HF_2wks_amp,HF_2wks_auc,1,HF_2wks_dur,1,HF_2wks_iei,HF_2wks_max,HF_2wks_mean,HF_2wks_min,HF_2wks_count,HF_2wks_std,2)\n",
    "df_HF12H=create_df(HF_12wks_amp,HF_12wks_auc,1,HF_12wks_dur,1,HF_12wks_iei,HF_12wks_max,HF_12wks_mean,HF_12wks_min,HF_12wks_count,HF_12wks_std,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "frames = [df_HF0H,df_HF2H,df_C2H,df_C12H,df_HF12H]\n",
    "df=pd.concat(frames)\n",
    "df=df_C0H.append(frames,ignore_index=True)\n",
    "df=df.dropna(axis=0)\n",
    "df_norm=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## if you want to restrict analysis to specific timepoint use this##\n",
    "df_norm=df_norm[((df_norm['timepoint']==0) & (df_norm['condition']==1))]#condition=1=hungry\n",
    "predict_outcome='group'  #which parameter do you want to predict (values should be 0 or 1)\n",
    "svm_filename='BL_0wks_pred_group'\n",
    "timepoint=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Use SVM to classify data based on 'group'\n",
    "iterations=1000\n",
    "test_data=.1  #defines split for test/train sets\n",
    "parameters = [{'C': [0.001, .01,.1,1,10,100,1000], 'gamma': [0.001, .01,.1,1,10,100,1000], 'kernel': ['rbf']}]\n",
    "\n",
    "pred_score=np.nan*np.zeros((iterations))\n",
    "rand_score=np.nan*np.zeros((iterations))\n",
    "for i in range(iterations):\n",
    "    clf = GridSearchCV(SVC(), parameters,cv=10)#,verbose=1000)\n",
    "    msk = np.random.rand(len(df_norm))>test_data\n",
    "    train = df_norm[msk]\n",
    "    train_labels=train[[predict_outcome]].values\n",
    "    c,r=train_labels.shape\n",
    "    train_labels=train_labels.reshape(c,)\n",
    "    clf.fit(train[['amp','auc','dur','iei','num events','min','max','mean']], train_labels)\n",
    "    print ('prediction accuracy = ', clf.best_score_)\n",
    "    pred_score[i]=(clf.best_score_)*100\n",
    "\n",
    "    ##random array of 1s and 0s for shuffled classifier\n",
    "    rand_group=pd.DataFrame(np.random.randint(2,size=train.shape[0]))\n",
    "    clf.fit(train[['amp','auc','dur','iei','num events','min','max','mean']], rand_group[0])\n",
    "    print ('shuffled accuracy = ', clf.best_score_)\n",
    "    rand_score[i]=(clf.best_score_)*100\n",
    "    print i\n",
    "\n",
    "d,p=stats.ks_2samp(pred_score,rand_score)\n",
    "plt.figure(figsize=(10,10))\n",
    "ax=plt.subplot(111)\n",
    "n = np.arange(1,len(pred_score)+1) / np.float(len(pred_score))\n",
    "Xs = np.sort(pred_score)\n",
    "plt.step(Xs,n,color='g',label='Model prediction',alpha=0.7,linewidth=2)\n",
    "n2= np.arange(1,len(rand_score)+1) / np.float(len(rand_score))\n",
    "Xs2= np.sort(rand_score)\n",
    "plt.step(Xs2,n,color='r',label='Random',alpha=0.7,linewidth=2)\n",
    "ax.set_axis_bgcolor('white')\n",
    "plt.xlabel('Prediction accuracy %',fontsize=20)\n",
    "plt.ylabel('Iterations',fontsize=20)\n",
    "plt.title('K-S test Results: '+str(\"D={0:.7f}\".format(round(d,3)))+' , '+str(\"p={0:.7f}\".format(round(p,3))),fontsize=12)\n",
    "plt.legend(fontsize=25,loc=2)\n",
    "plt.savefig(svm_filename+str(iterations)+str('iter_')+str(test_data)+str(timepoint)+'wks_test_data_prediction_accuracy_hist.png')\n",
    "np.savetxt(svm_filename+str(iterations)+str('iter_')+str(test_data)+str(timepoint)+'wks_test_data_prediction_accuracy.csv',pred_score,delimiter=',')\n",
    "np.savetxt(svm_filename+str(iterations)+str('iter_')+str(test_data)+str(timepoint)+'wks_test_data_RANDOM_prediction_accuracy.csv',rand_score,delimiter=',')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
